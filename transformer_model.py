import torch
from torch import nn 
import torch.nn.functional as F
from torch.nn import Transformer
import math
import random
import os

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# chia d·ªØ li·ªáu th√†nh t·ª´ng batch nh·ªè khi training
"""
l·ªõp chia batch th·ªß c√¥ng n√†y m√¨nh thi·∫øt k·∫ø ƒë·ªÉ t·ªëi ∆∞u vi·ªác ng·ªën ram c·ªßa tensor, thay v√¨ chuy·ªÉn t·∫•t c·∫£ batch
sang tensor th√¨ m√¨nh l·∫°i d√πng list, sau ƒë√≥ trong qu√° tr√¨nh training chia ra c√°c batch nh·ªè m√¨nh m·ªõi chuy·ªÉn
c√°c batch nh·ªè ƒë√≥ th√†nh tensor, ƒëi·ªÅu n√†y gi√∫p m√¥ h√¨nh d√π training tr√™n 1 t·∫≠p d·ªØ li·ªáu r·∫•t l·ªõn c≈©ng kh√¥ng m·∫•t
qu√° nhi·ªÅu ram
"""
class Dataset:
    def __init__(self, x_batch, y_batch, batch_num, random_batch=True):
        self.x_batch = x_batch
        self.y_batch = y_batch
        self.batch = []
        self.batch_num = batch_num
        self.random_batch = random_batch
        self.split_batch()

    def split_batch(self):
        for _ in range(math.ceil(len(self.x_batch)/self.batch_num)): # d√πng h√†m math.ceil ƒë·ªÉ l√†m tr√≤n s·ªë th·∫≠p ph√¢n sang s·ªë g·∫ßn nh·∫•t v√≠ d·ª•: 7.1 -> 8
            x_batch, y_batch = [], []
            for i in range(self.batch_num):
                if self.random_batch:
                    try:
                        self.x_batch[i]
                    except:
                        break
                    x_batch.append(random.choice(self.x_batch))
                    y_batch.append(random.choice(self.y_batch))
                else:
                    try:
                        x_batch.append(self.x_batch[i])
                        y_batch.append(self.y_batch[i])
                    except:
                        break
            self.batch.append((x_batch, y_batch))

# tokenizer c√°c k√Ω t·ª± trong c√¢u
class LanguageTokenizer(nn.Module):
    def __init__(self, x_batch, y_batch, max_sequence_length, pad="<pad>", end="<end>", start="<start>", out="<out>", embedding_file="embedding.pth"):
        super().__init__()
        self.x_batch =x_batch
        self.y_batch = y_batch
        self.vocab = [pad, end, start, out]
        self.get_vocab_batch()
        self.vocab_size = len(self.vocab)
        self.text_to_number = {v:k for k,v in enumerate(self.vocab)}
        self.number_to_text = {k:v for k,v in enumerate(self.vocab)}
        self.max_sequence_length = max_sequence_length
        self.pad = pad
        self.end = end
        self.start = start
        self.out = out
        self.__sub_run__()
    
    # ch·∫°y tr·ª±c ti·∫øp c√°c h√†m c·∫ßn thi·∫øt khi g·ªçi l·ªõp
    def __sub_run__(self):
        self.normalize_length_batch()
        self.tokenizer_batch()
        self.add_special_token()
        self.padding()
        self.normalize_2times_and_make_tensors()

    # l·ªõp nh·∫≠n t·ª´ v·ª±ng
    def get_vocab_batch(self):
        for vocab in list("".join(self.x_batch + self.y_batch)):
            if vocab not in self.vocab:
                self.vocab.append(vocab)
    
    # chu·∫©n h√≥a ƒë·ªô d√†i c·ªßa c√¢u
    def normalize_length_batch(self):
        for i in range(len(self.x_batch)):
            if len(list(self.x_batch[i])) > self.max_sequence_length:
                self.x_batch[i] = "".join(list(self.x_batch[i])[:self.max_sequence_length])
                self.y_batch[i] = "".join(list(self.y_batch[i])[:self.max_sequence_length])
    
    # chuy·ªÉn c√°c k√Ω t·ª± trong batch th√†nh tokens
    def tokenizer_batch(self):
        for i in range(len(self.x_batch)):
            self.x_batch[i] = [self.text_to_number[c] for c in list(self.x_batch[i])]
            self.y_batch[i] = [self.text_to_number[c] for c in list(self.y_batch[i])]
    
    # th√™m c√°c token ƒë·∫∑c bi·ªát v√†o ƒë·∫ßu v√† cu·ªëi c√¢u c·ªßa chu·ªïi m·ª•c ti√™u (Y)
    def add_special_token(self):
        for i in range(len(self.y_batch)):
            self.y_batch[i] = [2] + self.y_batch[i] + [1]
    
    # th√™m ƒë·ªám (s·ªë 0)
    def padding(self):
        for i in range(len(self.x_batch)):
            for _ in range(len(self.x_batch[i]), self.max_sequence_length):
                self.x_batch[i].append(0)
            for _ in range(len(self.y_batch[i]), self.max_sequence_length):
                self.y_batch[i].append(0) 
    
    # chu·∫©n h√≥a ƒë·ªô d√†i l·∫ßn th·ª© 2 ƒë·ªÉ ch·∫Øc r·∫±ng m·ªçi th·ª© ho·∫°t ƒë·ªông ·ªïn
    def normalize_2times_and_make_tensors(self):
        for i in range(len(self.x_batch)):
            if len(self.x_batch[i]) > self.max_sequence_length:
                self.x_batch[i] = self.x_batch[i][:self.max_sequence_length]
                self.y_batch[i] = self.y_batch[i][:self.max_sequence_length]
        self.x_batch = self.x_batch
        self.y_batch = self.y_batch
    
    # tokenizer ri√™ng cho c√¢u ƒë·∫ßu v√†o l·∫ª
    def tokenize_sentence(self, x):
        tokenize_sentence = []
        for token in list(x):
            try:
                tokenize_sentence.append(self.text_to_number[token])
            except:
                tokenize_sentence.append(self.text_to_number[self.out])
        tokenize_sentence = tokenize_sentence[:self.max_sequence_length]
        for _ in range(len(tokenize_sentence), self.max_sequence_length):
            tokenize_sentence.append(0)
        return torch.tensor([tokenize_sentence])

# m√¥ h√¨nh transformer
"""
m√¥ h√¨nh transformer m√¨nh d√πng m·∫∑c ƒë·ªãnh c·ªßa torch, n·∫øu mu·ªën xem chi ti·∫øt c√°ch x√¢y d·ª±ng model transformer
th√¨ m√¨nh c≈©ng c√≥ code s·∫≥n 1 m√¥ h√¨nh transformer t·ª´ ƒë·∫ßu, c·ª© v√†o ƒë∆∞·ªùng link github n√†y ƒë·ªÉ xem v√† b√≥c t√°ch
code üëá
https://github.com/PhuHoangTruong0208/Transformer-Finally/blob/main/transformers.py
"""
class TransformersGen(nn.Module):
    def __init__(self, hidden_dim, vocab_size, num_heads, num_enc_layers, num_dec_layers, dim_ffn, dropout, activation):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_dim).to(device)
        self.transformer = Transformer(hidden_dim, num_heads, num_enc_layers, num_dec_layers, dim_ffn, dropout, activation, batch_first=True, device=device)
        self.linear_out = nn.Linear(hidden_dim, vocab_size).to(device)
    
    def forward(self, x, y, argmax=False):
        x = self.embedding(x.to(device))
        y = self.embedding(y.to(device))
        x = self.transformer(x, y)
        x = self.linear_out(x)
        x = F.log_softmax(x, dim=-1)
        if argmax:
            x = torch.argmax(x, dim=-1)
        return x

# ph∆∞∆°ng th·ª©c ch√≠nh ch·ª©a c√°c ch·ª©c nƒÉng ch√≠nh
"""
ph∆∞∆°ng th·ª©c ch√≠nh, ch·ª©a t·∫•t c·∫£ c√°c l·ªõp ƒë√£ t·∫°o ·ªü tr√™n v√† ho·∫°t ƒë·ªông, c√≥ c·∫£ l·ªõp training, l·ªõp training n√†y m√¨nh
thi·∫øt k·∫ø ƒë·ªÉ cho n√≥ h·ªèi tr∆∞·ªõc khi traing, v√† ki·ªÉm tra model ƒë√£ l∆∞u hay ch∆∞a, b·∫°n c≈©ng c√≥ th·ªÉ ch·ªânh t√™n file
l∆∞u m√¥ h√¨nh b·∫±ng tham s·ªë (model_file_name).

l∆∞u √Ω m√£ n√†y ch·ªâ ƒë·ªÉ h·ªçc t·∫≠p, n·∫øu b·∫°n mu·ªën x√¢y 1 m√¥ h√¨nh ng√¥n ng·ªØ nh∆∞ GPT2 hay GPT3 nh∆∞ c·ªßa open AI
th√¨ v·∫´n ho√†n to√†n ƒë∆∞·ª£c, m√¥ h√¨nh n√†y r·∫•t m·∫°nh, b·∫°n ch·ªâ c·∫ßn ƒëi·ªÅu ch·ªânh c√°c tham s·ªë nh∆∞ hidden dim, dim_ffn,
num_head v√† th√™m v√†o m·ªôt s·ªë thu·∫≠t to√°n t√¨m k·∫øt qu·∫£ t√¥t nh·∫•t c·ªßa vector ƒë·∫ßu ra thay v√¨ argmax, v√≠ d·ª•: (beam search)...vv,
th√¨ n√≥ ho√†n to√†n ƒë·ªß s·ª©c ƒë·ªÉ ƒë·∫°t ƒë·∫øn m·ª©c ƒë·ªô ƒë√≥, nh∆∞ng L∆ØU √ù ·ªû ƒê√ÇY: n√≥ c·∫ßn r·∫•t r·∫•t nhi·ªÅu t√†i nguy√™n
t√≠nh to√°n v√† d·ªØ li·ªáu ƒë·ªÉ training! h√£y ch·∫Øc r·∫±ng b·∫°n c√≥ 1 si√™u m√°y t√≠nh ho·∫∑c 1 m√°y ch·ªß ph√¢n t√°n ƒë·ªÉ l√†m
ƒëi·ªÅu ƒë√≥ :P
"""
class MainTransformerSystem:
    def __init__(self, x_batch, y_batch, max_sequence_length, hidden_dim, num_heads=8, num_enc_layers=6, num_dec_layers=6,
                epoch=100, learning_rate=0.001, batch_size=16, random_batch=True, verbose=True, limit_loss_break_train=1,
                model_file_name="states.pth", dim_ffn=4096, dropout=0.01, activation=F.relu):
        self.tokenizer = LanguageTokenizer(x_batch, y_batch, max_sequence_length)
        self.transformer = TransformersGen(hidden_dim, self.tokenizer.vocab_size, num_heads, num_enc_layers, num_dec_layers, dim_ffn, dropout, activation)
        self.compute_loss = nn.CrossEntropyLoss(ignore_index=0)
        self.optimizer = torch.optim.Adam(self.transformer.parameters(), learning_rate)
        self.epochs = epoch
        self.batch_size = batch_size
        self.random_batch = random_batch
        self.verbose = verbose
        self.limit_loss_break_train = limit_loss_break_train
        self.model_file_name = model_file_name
        self.__sub_run__()
    
    # h·ªèi v√† ch·∫°y c√°c task theo l·ªánh
    def __sub_run__(self):
        if os.path.exists(self.model_file_name):
            inp = input("ƒë√£ c√≥ model ƒë√£ l∆∞u, b·∫°n c√≥ mu·ªën d√πng (mu·ªën/kh√¥ng) : ").lower().strip()
            if inp in "mu·ªën":
                self.transformer.load_state_dict(torch.load(self.model_file_name))
        inp = input("b·∫°n c√≥ mu·ªën training? (mu·ªën/kh√¥ng) : ").lower().strip()
        if inp in "mu·ªën":
            self.training()
    
    # l·ªõp ƒë√†o t·∫°o (training)
    def training(self):
        for epoch in range(self.epochs):
            dataset = Dataset(self.tokenizer.x_batch, self.tokenizer.y_batch, self.batch_size, random_batch=True)
            for i, (x_batch, y_batch) in enumerate(dataset.batch):
                # chuy·ªÉn c√°c batch nh·ªè (d·∫°ng list) th√†nh tensor
                x_batch = torch.tensor(x_batch)
                y_batch = torch.tensor(y_batch)
                self.transformer.train()
                self.optimizer.zero_grad()
                predict = self.transformer(x_batch, y_batch)
                loss = self.compute_loss(predict.view(-1, self.tokenizer.vocab_size).to(device), y_batch.view(-1).to(device))
                loss.backward(retain_graph=True)
                self.optimizer.step()
                self.loss = loss.item()
                if self.verbose:
                    print(f"Batch {i} Loss {loss.item()}")
            if self.verbose:
                print(f"complete {epoch}/{self.epochs}")
            if self.loss < self.limit_loss_break_train:
                if self.verbose:
                    self.transformer.to(torch.device('cpu'))
                    torch.save(self.transformer.state_dict(), self.model_file_name)
                    self.transformer.to(device)
                    print("m·∫•t m√°t ƒë√£ gi·∫£m ƒë√∫ng nh∆∞ s·ªë mong mu·ªën n√™n s·∫Ω d·ª´ng traning l·∫≠p t·ª©c")
                break
            self.transformer.to(torch.device('cpu'))
            torch.save(self.transformer.state_dict(), self.model_file_name)
            self.transformer.to(device)
    
    # chuy·ªÉn h√≥a c√°c tokens c·ªßa ƒë·∫ßu ra d·ª± ƒëo√°n sang text
    def decode_to_language(self, x):
        sentence = ""
        for token in x[0]:
            sentence += self.tokenizer.number_to_text[int(token)]
        return sentence
    
    # qu√° tr√¨nh ho·∫°t ƒë·ªông, d·ª± ƒëo√°n, chatting
    def chatting_predict(self, x):
        y = self.tokenizer.tokenize_sentence("<start>")
        x = self.tokenizer.tokenize_sentence(x)
        x = self.transformer(x, y, argmax=True)
        out = self.decode_to_language(x)
        return out

# m·∫´u
# x, y = ["hello", "what is your name"], ["hi", "my name is h"]
# model = MainTransformerSystem(x, y, max_sequence_length=300, hidden_dim=32, num_heads=4, num_dec_layers=2,
#                         num_enc_layers=2, dim_ffn=64, epoch=15, dropout=0.01, learning_rate=0.0001, limit_loss_break_train=0)
# result = model.chatting_predict("hello")
# print(result)